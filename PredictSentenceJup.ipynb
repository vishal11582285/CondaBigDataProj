{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import nltk as nlt\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from basefunctions import normalizeText, writeHighFreqTermsToFile\n",
    "from sample import load_model, readFromDisk, pickle_file_name_train, base_path_output, base_path_test, \\\n",
    "    sentence_file_name_train_neg, sentence_file_name_train_pos\n",
    "from wordembedtensor import kerasTokenizerUnit\n",
    "\n",
    "MAX_SENTENCE_LENGTH=1000\n",
    "topbestwords=1000\n",
    "\n",
    "\n",
    "\n",
    "def predictresp(model, finalSequenceUnitTokenizer,norm_text):\n",
    "    sequences = finalSequenceUnitTokenizer.texts_to_sequences(norm_text)\n",
    "    finalSequenceUnit = pad_sequences(sequences, maxlen=MAX_SENTENCE_LENGTH, padding='pre')\n",
    "    model_resp_prob = model.predict(finalSequenceUnit, verbose=0)\n",
    "    model_resp = model.predict_classes(finalSequenceUnit, verbose=0)\n",
    "    i = zip(model_resp, model_resp_prob)\n",
    "    return i\n",
    "\n",
    "def readFilesWithSentences(path, howManyFiles,global_nature):\n",
    "    fileNames = os.listdir(os.path.abspath(path))\n",
    "    fileContents = []\n",
    "    fileRatings=[]\n",
    "    a = 1\n",
    "    # print(fileNames[0:howManyFiles])\n",
    "    GroupLabel=[]\n",
    "    for current in fileNames[0:howManyFiles]:\n",
    "        with open(path + \"//\" + current, 'r', encoding=\"utf8\") as openFile:\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"%d[%-100s]%d  %d%%\" % (a,'=' * int(round((a/howManyFiles)*100,0)),howManyFiles, int(round((a/howManyFiles)*100,0))))\n",
    "            sys.stdout.flush()\n",
    "        #     # print(\"Currently Reading File : \" + currentFile + \" .Poll Progress:\" + \"(\" + str(a) + \" of \" + str(\n",
    "            readContent=openFile.readline()\n",
    "            fileContents.append(readContent)\n",
    "\n",
    "            stopWords = stopwords.words('english').remove('not')\n",
    "            stop = sorted(\n",
    "                stopWords + list(string.punctuation) + [\"i\\\\\", \"'m\", \"'s\", \"it\\\\\", '...', \"''\", '``', 'br',\n",
    "                                                        's', '--'])\n",
    "            tokens = nlt.tokenize.sent_tokenize(str(fileContents).lower())\n",
    "            tokens = [w for w in tokens if w not in stop]\n",
    "\n",
    "            abc=' '.join(tokens)\n",
    "            # global GroupLabel\n",
    "            rating=int(current[current.find(\"_\")+1:current.find(\".txt\")])\n",
    "            GroupLabel.append(abc)\n",
    "            fileRatings.append(rating)\n",
    "            a += 1\n",
    "    print(global_nature+ \" Files Read: %d\" % howManyFiles,end=\"\\n\")\n",
    "    return str(fileContents),GroupLabel,fileNames,fileRatings\n",
    "\n",
    "def processInputTest():\n",
    "    outputResult = open(base_path_output + \"/\" + \"outputResultTrain.txt\", 'w', encoding=\"utf8\")\n",
    "    positiveCorpus, positiveVectDict, wordFreqPos, GroupLabelPos, fileNamesPos, fileRatingsPos = writeHighFreqTermsToFile(\n",
    "        base_path_test + \"pos/\", outputResult, \"Positive\")\n",
    "    negativeCorpus, negativeVectDict, wordFreqNeg, GroupLabelNeg, fileNamesNeg, fileRatingsNeg = writeHighFreqTermsToFile(\n",
    "        base_path_test + \"neg/\", outputResult, \"Negative\")\n",
    "    outputResult.close()\n",
    "\n",
    "    allGroupKeys = GroupLabelPos + GroupLabelNeg\n",
    "    allGroupValues = fileRatingsPos + fileRatingsNeg\n",
    "    allFileNames = fileNamesPos + fileNamesNeg\n",
    "    allFileRatings = fileRatingsPos + fileRatingsNeg\n",
    "    # print(saveToDisk(allGroupKeys, allGroupValues, allFileNames, allFileRatings,fileName=pickle_file_name_test))\n",
    "    return allGroupKeys, allGroupValues, allFileNames, allFileRatings\n",
    "\n",
    "\n",
    "model=load_model()\n",
    "\n",
    "print(\"Reading from Pickle Object Saved.\",end=\"\\n\")\n",
    "allGroupKeysTrain, allGroupValuesTrain, allFileNamesTrain, allFileRatingsTrain=readFromDisk(pickle_file_name_train)\n",
    "finalSequenceUnitTokenizer = kerasTokenizerUnit(allGroupKeysTrain, MAX_SENTENCE_LENGTH, topbestwords)\n",
    "\n",
    "'''Reading from Pickle Objects:'''\n",
    "dataframeFromDiskPos = pd.read_pickle(base_path_output + sentence_file_name_train_pos)\n",
    "dataframeFromDiskNeg = pd.read_pickle(base_path_output + sentence_file_name_train_neg)\n",
    "fileContentsPos = []\n",
    "fileContentsNeg = []\n",
    "fileNamesPos = []\n",
    "fileNamesNeg = []\n",
    "# print(pd.DataFrame(dataframeFromDiskPos).shape)\n",
    "for a, b in zip(dataframeFromDiskPos[0], dataframeFromDiskPos[1]):\n",
    "    fileContentsPos.append([a])\n",
    "    fileNamesPos.append(b)\n",
    "for a,b in zip(dataframeFromDiskNeg[0], dataframeFromDiskNeg[1]):\n",
    "    fileContentsNeg.append([a])\n",
    "    fileNamesNeg.append(b)\n",
    "\n",
    "print(fileNamesNeg[0],fileNamesNeg[1])\n",
    "\n",
    "'''Reading from Pickle Objects:'''\n",
    "dataframeFromDiskPosVect = pd.read_pickle(base_path_output + 'savedPickleSentenceVectorPos.pickle')\n",
    "dataframeFromDiskNegVect = pd.read_pickle(base_path_output + 'savedPickleSentenceVectorNeg.pickle')\n",
    "\n",
    "# print(pd.DataFrame(dataframeFromDiskNegVect).head())\n",
    "# print(pd.DataFrame(dataframeFromDiskPosVect).shape)\n",
    "\n",
    "fileNamesNeg,vector_predict_sentence_classNeg, vector_predict_sentence_probNeg = [],[],[]\n",
    "fileNamesPos,vector_predict_sentence_classPos, vector_predict_sentence_probPos = [],[],[]\n",
    "\n",
    "for a,b,c in zip(dataframeFromDiskPosVect[0],dataframeFromDiskPosVect[1],dataframeFromDiskPosVect[2]):\n",
    "    fileNamesPos.append(a)\n",
    "    vector_predict_sentence_classPos.append(b)\n",
    "    vector_predict_sentence_probPos.append(c)\n",
    "for a,b,c in zip(dataframeFromDiskNegVect[0],dataframeFromDiskNegVect[1],dataframeFromDiskNegVect[2]):\n",
    "    fileNamesNeg.append(a)\n",
    "    vector_predict_sentence_classNeg.append(b)\n",
    "    vector_predict_sentence_probNeg.append(c)\n",
    "\n",
    "print(fileNamesPos[0],fileNamesPos[1])\n",
    "print(fileNamesNeg[0],fileNamesNeg[1])\n",
    "\n",
    "posSentenceCount=0\n",
    "print('Processing Positive Files',end='\\n')\n",
    "for x in range(0,len(fileContentsPos)):\n",
    "    temp=[k for i in fileContentsPos[x] for j in i for k in j]\n",
    "    sentence_split=str(temp[0]).split('.')\n",
    "    posSentenceCount+=len(sentence_split)-1\n",
    "\n",
    "negSentenceCount=0\n",
    "print('Processing Negative Files',end='\\n')\n",
    "for x in range(0,len(fileContentsNeg)):\n",
    "    temp=[k for i in fileContentsNeg[x] for j in i for k in j]\n",
    "    sentence_split=str(temp[0]).split('.')\n",
    "    negSentenceCount+=len(sentence_split)-1\n",
    "\n",
    "# print(posSentenceCount)\n",
    "# print(negSentenceCount)\n",
    "\n",
    "predicted=np.zeros(len(fileNamesPos)+len(fileContentsNeg))\n",
    "\n",
    "for i in range(0,len(predicted)):\n",
    "    if(i<12500):\n",
    "        # print(fileNamesPos[i])\n",
    "        a=np.array(vector_predict_sentence_probPos[i])\n",
    "    # print(a,end='\\n')\n",
    "        if len(a)!=0:\n",
    "            temp=(np.sum(a)/len(a))\n",
    "            predicted[i]=1 if temp>0.5 else 0\n",
    "    else:\n",
    "        # print(fileNamesNeg[i-12500])\n",
    "        a = np.array(vector_predict_sentence_probNeg[i-12500])\n",
    "        # print(a,end='\\n')\n",
    "        if len(a)!=0:\n",
    "            temp=(np.sum(a)/len(a))\n",
    "            predicted[i]=1 if temp>0.6 else 0\n",
    "        # temp = (np.sum(a) / len(a))\n",
    "        # predicted[i] = 1 if temp > 0.5 else 0\n",
    "    # print(vector_predict_sentence_classPos[i],end='\\n')\n",
    "    # # print(len(vector_predict_sentence_classPos[i]))\n",
    "\n",
    "print(Counter(predicted))\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(0,len(allFileNamesTrain)):\n",
    "#     print(str(allFileNamesTrain[i])+':'+str(predicted[i]))\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "# print(len(fileContentsPos))\n",
    "# print(len(fileContentsNeg))\n",
    "\n",
    "# dataframeFromDiskPos=pd.DataFrame(dataframeFromDiskPos)\n",
    "# print(dataframeFromDiskPos.shape)\n",
    "\n",
    "# vector_predict_sentence_class = []\n",
    "# vector_predict_sentence_prob = []\n",
    "# temp1 = []\n",
    "# temp2 = []\n",
    "# posSentenceCount = 0\n",
    "# print('Processing Positive Files', end='\\n')\n",
    "# for x in range(0, len(fileContentsPos)):\n",
    "#     temp = [k for i in fileContentsPos[x] for j in i for k in j]\n",
    "#     sentence_split = str(temp[0]).split('.')\n",
    "#     sys.stdout.write('\\r')\n",
    "#     sys.stdout.write(\"%d[%-100s]%d  %d%%\" % (\n",
    "#         x, '=' * int(round((x / len(fileContentsPos)) * 100, 0)), len(fileContentsPos),\n",
    "#         int(round((x / len(fileContentsPos)) * 100, 0))))\n",
    "#     sys.stdout.flush()\n",
    "#     posSentenceCount += len(sentence_split) - 1\n",
    "#     temp1 = []\n",
    "#     temp2 = []\n",
    "#     for i in sentence_split[0:len(sentence_split) - 1]:\n",
    "#         norm_text = normalizeText(i)\n",
    "#         # print(norm_text)\n",
    "#         j = predictresp(model, finalSequenceUnitTokenizer, [norm_text])\n",
    "#         for a, b in list(j):\n",
    "#             sentence_class = int(a)\n",
    "#             sentence_prob = float(b)\n",
    "#         temp1.append(sentence_class)\n",
    "#         temp2.append(sentence_prob)\n",
    "#     vector_predict_sentence_class.append(temp1)\n",
    "#     vector_predict_sentence_prob.append(temp2)\n",
    "#     # print(vector_predict_sentence_class)\n",
    "#     # print(vector_predict_sentence_prob)\n",
    "#\n",
    "# listToWrite = []\n",
    "#\n",
    "# for i, j, k in zip(fileNamesPos, vector_predict_sentence_class, vector_predict_sentence_prob):\n",
    "#     listToWrite.append([i, j, k, 1])\n",
    "# dataframeToDisk = pd.DataFrame(listToWrite)\n",
    "# print(dataframeToDisk.shape)\n",
    "# print(dataframeToDisk.head())\n",
    "# # print(dataframeToDisk[1])\n",
    "# dataframeToDisk.to_pickle(base_path_output + 'savedPickleSentenceVectorPos.pickle')\n",
    "# print(\"Successfully Written Data to Disk (Pickle Object) !\")\n",
    "#\n",
    "# print(posSentenceCount)\n",
    "#\n",
    "# print('Processing Negative Files', end='\\n')\n",
    "# vector_predict_sentence_class = []\n",
    "# vector_predict_sentence_prob = []\n",
    "# temp1 = []\n",
    "# temp2 = []\n",
    "# negSentenceCount = 0\n",
    "# for x in range(0, len(fileNamesNeg)):\n",
    "#     temp = [k for i in fileContentsNeg[x] for j in i for k in j]\n",
    "#     sentence_split = str(temp[0]).split('.')\n",
    "#     sys.stdout.write('\\r')\n",
    "#     sys.stdout.write(\"%d[%-100s]%d  %d%%\" % (\n",
    "#         x, '=' * int(round((x / len(fileContentsNeg)) * 100, 0)), len(fileContentsNeg),\n",
    "#         int(round((x / len(fileContentsNeg)) * 100, 0))))\n",
    "#     sys.stdout.flush()\n",
    "#     negSentenceCount += len(sentence_split) - 1\n",
    "#     temp1 = []\n",
    "#     temp2 = []\n",
    "#     for i in sentence_split[0:len(sentence_split) - 1]:\n",
    "#         norm_text = normalizeText(i)\n",
    "#         # print(norm_text)\n",
    "#         j = predictresp(model, finalSequenceUnitTokenizer, [norm_text])\n",
    "#         for a, b in list(j):\n",
    "#             sentence_class = int(a)\n",
    "#             sentence_prob = float(b)\n",
    "#         temp1.append(sentence_class)\n",
    "#         temp2.append(sentence_prob)\n",
    "#     vector_predict_sentence_class.append(temp1)\n",
    "#     vector_predict_sentence_prob.append(temp2)\n",
    "#\n",
    "# print(fileNamesNeg[0],fileNamesNeg[1])\n",
    "# listToWrite = []\n",
    "# for i, j, k in zip(fileNamesNeg, vector_predict_sentence_class, vector_predict_sentence_prob):\n",
    "#     listToWrite.append([i, j, k, 0])\n",
    "# dataframeToDisk = pd.DataFrame(listToWrite)\n",
    "# print(dataframeToDisk.shape)\n",
    "# print(dataframeToDisk.head())\n",
    "# dataframeToDisk.to_pickle(base_path_output + 'savedPickleSentenceVectorNeg.pickle')\n",
    "# print(\"Successfully Written Data to Disk (Pickle Object) !\")\n",
    "#\n",
    "# print(negSentenceCount)\n",
    "\n",
    "# N= posSentenceCount + negSentenceCount\n",
    "# print(N)\n",
    "'''\n",
    "Implementing cost function:\n",
    "\n",
    "N=sentences\n",
    "lambda=any number > 0\n",
    "K=no of groups\n",
    "\n",
    "yi=yteta=from CNN predictions\n",
    "yj=from CNN predictions\n",
    "\n",
    "\n",
    "Kernel=exp(−kxi − xjk2 2) : pairwise_sq_dists = squareform(pdist(X, 'sqeuclidean'))\n",
    "K = scip.exp(-pairwise_sq_dists / s**2)\n",
    "\n",
    "lk=summation(yteta) : positive, else 0\n",
    "\n",
    "1) Find number of sentences : positve, negative\n",
    "2) We know groups: pos=12500 neg=12500\n",
    "3) \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nlt\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from basefunctions import normalizeText, writeHighFreqTermsToFile\n",
    "from sample import load_model, readFromDisk, pickle_file_name_train, base_path_output, base_path_test, \\\n",
    "    sentence_file_name_train_neg, sentence_file_name_train_pos\n",
    "from wordembedtensor import kerasTokenizerUnit\n",
    "\n",
    "MAX_SENTENCE_LENGTH=1000\n",
    "topbestwords=1000\n",
    "\n",
    "\n",
    "\n",
    "def predictresp(model, finalSequenceUnitTokenizer,norm_text):\n",
    "    sequences = finalSequenceUnitTokenizer.texts_to_sequences(norm_text)\n",
    "    finalSequenceUnit = pad_sequences(sequences, maxlen=MAX_SENTENCE_LENGTH, padding='pre')\n",
    "    model_resp_prob = model.predict(finalSequenceUnit, verbose=0)\n",
    "    model_resp = model.predict_classes(finalSequenceUnit, verbose=0)\n",
    "    i = zip(model_resp, model_resp_prob)\n",
    "    return i\n",
    "\n",
    "def readFilesWithSentences(path, howManyFiles,global_nature):\n",
    "    fileNames = os.listdir(os.path.abspath(path))\n",
    "    fileContents = []\n",
    "    fileRatings=[]\n",
    "    a = 1\n",
    "    # print(fileNames[0:howManyFiles])\n",
    "    GroupLabel=[]\n",
    "    for current in fileNames[0:howManyFiles]:\n",
    "        with open(path + \"//\" + current, 'r', encoding=\"utf8\") as openFile:\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"%d[%-100s]%d  %d%%\" % (a,'=' * int(round((a/howManyFiles)*100,0)),howManyFiles, int(round((a/howManyFiles)*100,0))))\n",
    "            sys.stdout.flush()\n",
    "        #     # print(\"Currently Reading File : \" + currentFile + \" .Poll Progress:\" + \"(\" + str(a) + \" of \" + str(\n",
    "            readContent=openFile.readline()\n",
    "            fileContents.append(readContent)\n",
    "\n",
    "            stopWords = stopwords.words('english').remove('not')\n",
    "            stop = sorted(\n",
    "                stopWords + list(string.punctuation) + [\"i\\\\\", \"'m\", \"'s\", \"it\\\\\", '...', \"''\", '``', 'br',\n",
    "                                                        's', '--'])\n",
    "            tokens = nlt.tokenize.sent_tokenize(str(fileContents).lower())\n",
    "            tokens = [w for w in tokens if w not in stop]\n",
    "\n",
    "            abc=' '.join(tokens)\n",
    "            # global GroupLabel\n",
    "            rating=int(current[current.find(\"_\")+1:current.find(\".txt\")])\n",
    "            GroupLabel.append(abc)\n",
    "            fileRatings.append(rating)\n",
    "            a += 1\n",
    "    print(global_nature+ \" Files Read: %d\" % howManyFiles,end=\"\\n\")\n",
    "    return str(fileContents),GroupLabel,fileNames,fileRatings\n",
    "\n",
    "def processInputTest():\n",
    "    outputResult = open(base_path_output + \"/\" + \"outputResultTrain.txt\", 'w', encoding=\"utf8\")\n",
    "    positiveCorpus, positiveVectDict, wordFreqPos, GroupLabelPos, fileNamesPos, fileRatingsPos = writeHighFreqTermsToFile(\n",
    "        base_path_test + \"pos/\", outputResult, \"Positive\")\n",
    "    negativeCorpus, negativeVectDict, wordFreqNeg, GroupLabelNeg, fileNamesNeg, fileRatingsNeg = writeHighFreqTermsToFile(\n",
    "        base_path_test + \"neg/\", outputResult, \"Negative\")\n",
    "    outputResult.close()\n",
    "\n",
    "    allGroupKeys = GroupLabelPos + GroupLabelNeg\n",
    "    allGroupValues = fileRatingsPos + fileRatingsNeg\n",
    "    allFileNames = fileNamesPos + fileNamesNeg\n",
    "    allFileRatings = fileRatingsPos + fileRatingsNeg\n",
    "    # print(saveToDisk(allGroupKeys, allGroupValues, allFileNames, allFileRatings,fileName=pickle_file_name_test))\n",
    "    return allGroupKeys, allGroupValues, allFileNames, allFileRatings\n",
    "\n",
    "\n",
    "model=load_model()\n",
    "\n",
    "print(\"Reading from Pickle Object Saved.\",end=\"\\n\")\n",
    "allGroupKeysTrain, allGroupValuesTrain, allFileNamesTrain, allFileRatingsTrain=readFromDisk(pickle_file_name_train)\n",
    "finalSequenceUnitTokenizer = kerasTokenizerUnit(allGroupKeysTrain, MAX_SENTENCE_LENGTH, topbestwords)\n",
    "\n",
    "'''Reading from Pickle Objects:'''\n",
    "dataframeFromDiskPos = pd.read_pickle(base_path_output + sentence_file_name_train_pos)\n",
    "dataframeFromDiskNeg = pd.read_pickle(base_path_output + sentence_file_name_train_neg)\n",
    "fileContentsPos = []\n",
    "fileContentsNeg = []\n",
    "fileNamesPos = []\n",
    "fileNamesNeg = []\n",
    "# print(pd.DataFrame(dataframeFromDiskPos).shape)\n",
    "for a, b in zip(dataframeFromDiskPos[0], dataframeFromDiskPos[1]):\n",
    "    fileContentsPos.append([a])\n",
    "    fileNamesPos.append(b)\n",
    "for a,b in zip(dataframeFromDiskNeg[0], dataframeFromDiskNeg[1]):\n",
    "    fileContentsNeg.append([a])\n",
    "    fileNamesNeg.append(b)\n",
    "\n",
    "print(fileNamesNeg[0],fileNamesNeg[1])\n",
    "\n",
    "'''Reading from Pickle Objects:'''\n",
    "dataframeFromDiskPosVect = pd.read_pickle(base_path_output + 'savedPickleSentenceVectorPos.pickle')\n",
    "dataframeFromDiskNegVect = pd.read_pickle(base_path_output + 'savedPickleSentenceVectorNeg.pickle')\n",
    "\n",
    "# print(pd.DataFrame(dataframeFromDiskNegVect).head())\n",
    "# print(pd.DataFrame(dataframeFromDiskPosVect).shape)\n",
    "\n",
    "fileNamesNeg,vector_predict_sentence_classNeg, vector_predict_sentence_probNeg = [],[],[]\n",
    "fileNamesPos,vector_predict_sentence_classPos, vector_predict_sentence_probPos = [],[],[]\n",
    "\n",
    "for a,b,c in zip(dataframeFromDiskPosVect[0],dataframeFromDiskPosVect[1],dataframeFromDiskPosVect[2]):\n",
    "    fileNamesPos.append(a)\n",
    "    vector_predict_sentence_classPos.append(b)\n",
    "    vector_predict_sentence_probPos.append(c)\n",
    "for a,b,c in zip(dataframeFromDiskNegVect[0],dataframeFromDiskNegVect[1],dataframeFromDiskNegVect[2]):\n",
    "    fileNamesNeg.append(a)\n",
    "    vector_predict_sentence_classNeg.append(b)\n",
    "    vector_predict_sentence_probNeg.append(c)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
